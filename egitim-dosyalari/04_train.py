"""
Model EÄŸitimi - Kaggle Optimized
50,000 gÃ¶rÃ¼ntÃ¼, <12 saat, %85+ AUC hedefi
Focal Loss + Class Weights + Cosine Annealing + Backbone Freezing
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.amp import autocast, GradScaler
import numpy as np
from sklearn.metrics import roc_auc_score
from tqdm import tqdm
from pathlib import Path
import time
import config
from model import MultimodalChestXrayModel
from dataset import create_dataloaders


class FocalLoss(nn.Module):
    """
    Focal Loss for multi-label classification
    Helps with class imbalance
    """
    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight  # Class weights

    def forward(self, inputs, targets):
        # BCE with logits
        bce_loss = nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none', pos_weight=self.pos_weight
        )

        # Focal term
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss

        return focal_loss.mean()


class Trainer:
    """
    Enhanced Trainer with:
    - Focal Loss + Class Weights
    - Cosine Annealing + Warmup
    - Backbone Freezing
    - Mixed Precision
    - Early Stopping
    """
    def __init__(self, model, train_loader, val_loader, device):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device

        # Class weights for imbalanced data
        self.class_weights = self._compute_class_weights()

        # Loss function
        if config.USE_FOCAL_LOSS:
            self.criterion = FocalLoss(
                alpha=config.FOCAL_LOSS_ALPHA,
                gamma=config.FOCAL_LOSS_GAMMA,
                pos_weight=self.class_weights
            )
        else:
            self.criterion = nn.BCEWithLogitsLoss(pos_weight=self.class_weights)

        # Optimizer
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )

        # Learning rate scheduler
        if config.USE_COSINE_ANNEALING:
            # T_max hesapla: Toplam epoch - freeze epoch - warmup epoch
            # Ã–rnek: 14 - 3 - 2 = 9 epoch iÃ§in cosine annealing
            effective_epochs = max(1, config.EPOCHS - config.FREEZE_BACKBONE_EPOCHS - config.WARMUP_EPOCHS)
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=effective_epochs,  # âœ… DÃœZELTÄ°LDÄ°!
                eta_min=config.LR_MIN
            )
            self.warmup_epochs = config.WARMUP_EPOCHS
            print(f"  ğŸ“‰ CosineAnnealingLR: T_max={effective_epochs} (epochs {config.FREEZE_BACKBONE_EPOCHS+config.WARMUP_EPOCHS+1}-{config.EPOCHS})")
        else:
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='max',
                factor=0.5,
                patience=3,
                min_lr=config.LR_MIN,
                verbose=True
            )
            self.warmup_epochs = 0

        # Mixed precision
        self.scaler = GradScaler('cuda') if config.USE_AMP else None

        # Tracking
        self.best_val_auc = 0
        self.patience_counter = 0
        self.train_losses = []
        self.val_losses = []
        self.val_aucs = []
        self.start_time = time.time()

    def _compute_class_weights(self):
        """Compute class weights from config.DISTRIBUTION"""
        if not config.USE_CLASS_WEIGHTS:
            return None

        weights = []
        total = sum(config.DISTRIBUTION.values())

        for disease in config.DISEASES:
            count = config.DISTRIBUTION[disease]
            # Inverse frequency weight
            weight = total / (count * config.NUM_DISEASES)
            weights.append(weight)

        weights_tensor = torch.FloatTensor(weights).to(self.device)

        print(f"\nâš–ï¸  Class Weights:")
        for disease, weight in zip(config.DISEASES, weights):
            print(f"  {disease:20s}: {weight:.3f}")

        return weights_tensor

    def train_epoch(self, epoch):
        """Train one epoch"""
        self.model.train()
        running_loss = 0.0
        all_preds = []
        all_labels = []

        # Warmup learning rate
        if epoch < self.warmup_epochs:
            lr_scale = (epoch + 1) / self.warmup_epochs
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = config.LEARNING_RATE * lr_scale

        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{config.EPOCHS} [TRAIN]')

        for batch in pbar:
            images = batch['image'].to(self.device)
            demographics = batch['demographics'].to(self.device)
            labels = batch['labels'].to(self.device)

            self.optimizer.zero_grad()

            # Forward pass with mixed precision
            if config.USE_AMP:
                with autocast('cuda'):
                    logits = self.model(images, demographics)
                    loss = self.criterion(logits, labels)

                self.scaler.scale(loss).backward()
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                logits = self.model(images, demographics)
                loss = self.criterion(logits, labels)
                loss.backward()
                self.optimizer.step()

            running_loss += loss.item()

            # Predictions for AUC
            preds = torch.sigmoid(logits).detach().cpu().numpy()
            all_preds.append(preds)
            all_labels.append(labels.cpu().numpy())

            pbar.set_postfix({'loss': f'{loss.item():.4f}'})

        # Epoch metrics
        epoch_loss = running_loss / len(self.train_loader)
        all_preds = np.vstack(all_preds)
        all_labels = np.vstack(all_labels)

        try:
            epoch_auc = roc_auc_score(all_labels, all_preds, average='macro')
        except:
            epoch_auc = 0.0

        return epoch_loss, epoch_auc

    def validate(self):
        """Validation"""
        self.model.eval()
        running_loss = 0.0
        all_preds = []
        all_labels = []

        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validating'):
                images = batch['image'].to(self.device)
                demographics = batch['demographics'].to(self.device)
                labels = batch['labels'].to(self.device)

                logits = self.model(images, demographics)
                loss = self.criterion(logits, labels)

                running_loss += loss.item()

                preds = torch.sigmoid(logits).cpu().numpy()
                all_preds.append(preds)
                all_labels.append(labels.cpu().numpy())

        val_loss = running_loss / len(self.val_loader)
        all_preds = np.vstack(all_preds)
        all_labels = np.vstack(all_labels)

        try:
            val_auc = roc_auc_score(all_labels, all_preds, average='macro')
        except:
            val_auc = 0.0

        return val_loss, val_auc

    def train(self, num_epochs):
        """Main training loop"""
        print("\n" + "="*70)
        print(f"ğŸš€ EÄÄ°TÄ°M BAÅLIYOR ({config.TOTAL_IMAGES//1000}K GÃ–RÃœNTÃœ)")
        print("="*70)
        print(f"Epochs: {num_epochs}")
        print(f"Batch size: {config.BATCH_SIZE}")
        print(f"Learning rate: {config.LEARNING_RATE}")
        print(f"Weight decay: {config.WEIGHT_DECAY}")
        print(f"Dropout: {config.DROPOUT_RATE}")
        print(f"Device: {self.device}")
        print(f"Mixed Precision: {'âœ…' if config.USE_AMP else 'âŒ'}")
        print(f"Focal Loss: {'âœ…' if config.USE_FOCAL_LOSS else 'âŒ'}")
        print(f"Class Weights: {'âœ…' if config.USE_CLASS_WEIGHTS else 'âŒ'}")
        print(f"Cosine Annealing: {'âœ…' if config.USE_COSINE_ANNEALING else 'âŒ'}")
        print(f"Backbone Freeze: {config.FREEZE_BACKBONE_EPOCHS} epochs")
        print(f"Train batches: {len(self.train_loader):,}")
        print(f"Val batches: {len(self.val_loader):,}")
        print(f"\nğŸ¯ Hedef: AUC > {config.EXPECTED_PERFORMANCE['target_auc']:.2f}")
        print(f"â±ï¸  Tahmini sÃ¼re: ~{config.EXPECTED_PERFORMANCE['training_time_hours']:.1f} saat")
        print(f"â°  Maksimum sÃ¼re: 12 saat")
        print("="*70 + "\n")

        # Checkpoint directory
        checkpoint_dir = Path(config.MODELS_DIR)
        checkpoint_dir.mkdir(exist_ok=True, parents=True)

        for epoch in range(num_epochs):
            # Time limit check (12 hours)
            elapsed_hours = (time.time() - self.start_time) / 3600
            if elapsed_hours > 11.5:
                print(f"\nâ° SÃœRE LÄ°MÄ°TÄ°: {elapsed_hours:.1f} saat geÃ§ti, eÄŸitim durduruluyor...")
                break

            # Unfreeze backbone after initial epochs
            if epoch == config.FREEZE_BACKBONE_EPOCHS:
                self.model.unfreeze_backbone()

                # Reset optimizer with new parameters
                self.optimizer = optim.AdamW(
                    self.model.parameters(),
                    lr=config.LEARNING_RATE,
                    weight_decay=config.WEIGHT_DECAY
                )

                # âœ… FIX: Scheduler'Ä± da yenile! (Gemini'nin tespit ettiÄŸi bug!)
                if config.USE_COSINE_ANNEALING:
                    # T_max: Kalan epoch sayÄ±sÄ± (warmup hariÃ§)
                    remaining_epochs = config.EPOCHS - epoch - 1  # Epoch 3'ten sonra kalan
                    effective_T = max(1, remaining_epochs)
                    self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                        self.optimizer,
                        T_max=effective_T,  # âœ… DOÄRU HESAPLAMA!
                        eta_min=config.LR_MIN
                    )
                    print(f"  ğŸ“‰ CosineAnnealingLR reset: T_max={effective_T} (epochs {epoch+2}-{config.EPOCHS})")
                else:
                    self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                        self.optimizer,
                        mode='max',
                        factor=0.5,
                        patience=3,
                        verbose=True
                    )

                print(f"  âœ… Optimizer VE Scheduler yenilendi (LR: {config.LEARNING_RATE})")

            # Train
            train_loss, train_auc = self.train_epoch(epoch)
            self.train_losses.append(train_loss)

            # Validate
            val_loss, val_auc = self.validate()
            self.val_losses.append(val_loss)
            self.val_aucs.append(val_auc)

            # Scheduler step
            if config.USE_COSINE_ANNEALING:
                if epoch >= self.warmup_epochs:
                    self.scheduler.step()
            else:
                self.scheduler.step(val_auc)

            current_lr = self.optimizer.param_groups[0]['lr']

            # Progress
            elapsed = (time.time() - self.start_time) / 60
            overfitting_gap = train_auc - val_auc

            print(f"\nEpoch {epoch+1}/{num_epochs} - {elapsed:.1f}m ({elapsed/60:.1f}h)")
            print(f"  Train Loss: {train_loss:.4f} | Train AUC: {train_auc:.4f}")
            print(f"  Val Loss:   {val_loss:.4f} | Val AUC:   {val_auc:.4f}")
            print(f"  Overfitting Gap: {overfitting_gap:.4f} {'âš ï¸' if overfitting_gap > 0.05 else 'âœ…'}")
            print(f"  LR: {current_lr:.2e}")

            # Best model
            if val_auc > self.best_val_auc:
                self.best_val_auc = val_auc
                self.patience_counter = 0

                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_auc': val_auc,
                    'train_auc': train_auc,
                    'val_loss': val_loss,
                    'train_loss': train_loss,
                    'config': {
                        'num_diseases': config.NUM_DISEASES,
                        'demographic_features': config.NUM_DEMOGRAPHIC_FEATURES,
                        'dropout': config.DROPOUT_RATE
                    }
                }, checkpoint_dir / 'best_model.pth')

                print(f"  ğŸ† En iyi model kaydedildi! AUC: {val_auc:.4f}")
            else:
                self.patience_counter += 1

            # Periodic checkpoint
            if (epoch + 1) % 5 == 0:
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_auc': val_auc
                }, checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pth')
                print(f"  ğŸ’¾ Checkpoint kaydedildi: epoch_{epoch+1}.pth")

            # Early stopping
            if self.patience_counter >= config.EARLY_STOP_PATIENCE:
                print(f"\nâš ï¸  Early stopping! {config.EARLY_STOP_PATIENCE} epoch iyileÅŸme yok.")
                break

        # Final summary
        total_time = (time.time() - self.start_time) / 60
        print("\n" + "="*70)
        print("âœ… EÄÄ°TÄ°M TAMAMLANDI!")
        print("="*70)
        print(f"Toplam sÃ¼re: {total_time:.1f} dakika ({total_time/60:.1f} saat)")
        print(f"En iyi Val AUC: {self.best_val_auc:.4f} (Epoch {np.argmax(self.val_aucs)+1})")

        # Target check
        if self.best_val_auc >= config.EXPECTED_PERFORMANCE['target_auc']:
            print(f"ğŸ¯ HEDEF BAÅARILI: {self.best_val_auc:.4f} >= {config.EXPECTED_PERFORMANCE['target_auc']:.2f} âœ…")
        else:
            print(f"âš ï¸  Hedefin altÄ±nda: {self.best_val_auc:.4f} < {config.EXPECTED_PERFORMANCE['target_auc']:.2f}")
            print(f"   Fark: {config.EXPECTED_PERFORMANCE['target_auc'] - self.best_val_auc:.4f}")

        print("="*70)


def main():
    """Main function"""
    # Config info
    print("\n" + "="*70)
    print("âš™ï¸  KONFÄ°GÃœRASYON")
    print("="*70)
    print(f"Proje: {config.PROJECT_NAME} v{config.VERSION}")
    print(f"Veri: {config.TOTAL_IMAGES:,} gÃ¶rÃ¼ntÃ¼")
    print(f"HastalÄ±k sayÄ±sÄ±: {config.NUM_DISEASES}")
    print(f"Device: {config.DEVICE}")
    print("="*70)

    # DataLoaders
    csv_suffix = f"{config.TOTAL_IMAGES//1000}k"
    train_csv = Path(config.OUTPUT_DIR) / f'train_{csv_suffix}.csv'
    val_csv = Path(config.OUTPUT_DIR) / f'val_{csv_suffix}.csv'
    test_csv = Path(config.OUTPUT_DIR) / f'test_{csv_suffix}.csv'

    train_loader, val_loader, _ = create_dataloaders(
        train_csv=str(train_csv),
        val_csv=str(val_csv),
        test_csv=str(test_csv),
        img_dir=config.IMAGES_BASE_DIR,
        batch_size=config.BATCH_SIZE
    )

    # Model
    print("\n" + "="*70)
    print("ğŸ—‚ï¸  MODEL OLUÅTURULUYOR")
    print("="*70)

    model = MultimodalChestXrayModel(
        num_diseases=config.NUM_DISEASES,
        demographic_features=config.NUM_DEMOGRAPHIC_FEATURES,
        pretrained=True,
        dropout=config.DROPOUT_RATE,
        use_attention=True  # âœ… Attention fusion (daha akÄ±llÄ±!)
    )

    # Freeze backbone initially
    if config.FREEZE_BACKBONE_EPOCHS > 0:
        model.freeze_backbone()

    # Multi-GPU DEVRE DIÅI (overhead Ã§ok fazla, tek GPU daha hÄ±zlÄ±!)
    num_gpus = torch.cuda.device_count()
    print(f"\n{'='*70}")
    print(f"ğŸ¯ TEK GPU MODU (Overhead yok, daha hÄ±zlÄ±!)")
    print(f"{'='*70}")
    print(f"  Mevcut GPU: {num_gpus} adet")
    print(f"  KullanÄ±lacak: 1 GPU (DataParallel overhead Ã¶nlendi!)")
    print(f"  Batch Size: {config.BATCH_SIZE} (tek GPU'ya)")
    print(f"  âœ… Tek GPU aktif!")
    print(f"{'='*70}\n")
    
    # Multi-GPU kullanma (overhead Ã§ok fazla!)
    # if num_gpus > 1:
    #     model = nn.DataParallel(model)

    model = model.to(config.DEVICE)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"âœ“ Model oluÅŸturuldu: {config.PRETRAINED_MODEL}")
    print(f"  Toplam parametreler: {total_params:,}")
    print(f"  EÄŸitilebilir: {trainable_params:,}")
    print("="*70)

    # Train
    trainer = Trainer(model, train_loader, val_loader, config.DEVICE)
    trainer.train(num_epochs=config.EPOCHS)

    print(f"\nâœ… EÄŸitim tamamlandÄ±!")
    print(f"ğŸ“‚ Model: {config.MODELS_DIR}/best_model.pth")
    print(f"ğŸ¯ En iyi AUC: {trainer.best_val_auc:.4f}")


if __name__ == '__main__':
    main()
