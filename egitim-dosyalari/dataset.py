"""
MULTIMODAL DATASET - G√∂r√ºnt√º + Demografik Veri
Kaggle NIH Chest X-ray Dataset i√ßin optimize edilmi≈ü
Multi-label classification + Patient-level split
"""

import torch
from torch.utils.data import Dataset
from PIL import Image
import pandas as pd
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2
from pathlib import Path
import config


class ChestXrayMultimodalDataset(Dataset):
    """
    Chest X-ray g√∂r√ºnt√ºleri + demografik bilgiler
    Multi-label classification i√ßin
    """

    def __init__(self, csv_file, img_dir, mode='train'):
        """
        Args:
            csv_file: CSV dosya yolu (train/val/test)
            img_dir: G√∂r√ºnt√º base dizini
            mode: 'train', 'val', veya 'test'
        """
        self.df = pd.read_csv(csv_file)
        self.img_base_dir = Path(img_dir)
        self.mode = mode
        self.diseases = config.DISEASES
        self.transform = self._get_transforms()

        # G√∂r√ºnt√º dizinlerini √∂n y√ºkle
        self.image_dirs = self._get_image_directories()

        print(f"‚úì {mode.upper()} Dataset y√ºklendi: {len(self.df):,} g√∂r√ºnt√º")
        print(f"  G√∂r√ºnt√º klas√∂rleri: {len(self.image_dirs)} adet")

        # Multi-label statistics
        self._print_label_statistics()

    def _get_image_directories(self):
        """Kaggle: images_001/images/, images_002/images/, ... images_012/images/"""
        dirs = []

        # images_001 ~ images_012
        for i in range(1, 13):
            subdir = self.img_base_dir / f'images_{i:03d}' / 'images'
            if subdir.exists():
                dirs.append(subdir)

        if len(dirs) == 0:
            # Fallback: Belki direkt images/ klas√∂r√º var?
            fallback = self.img_base_dir / 'images'
            if fallback.exists():
                dirs.append(fallback)

        return sorted(dirs)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_filename = row['Image Index']

        # G√∂r√ºnt√ºy√º y√ºkle
        try:
            img_path = self._find_image_path(img_filename)
            image = Image.open(img_path).convert('RGB')
            image = np.array(image)
        except Exception as e:
            # Hata durumunda siyah g√∂r√ºnt√º (nadiren olur)
            print(f"‚ö†Ô∏è  {img_filename}: {e}")
            image = np.zeros((224, 224, 3), dtype=np.uint8)

        # Transform uygula
        if self.transform:
            image = self.transform(image=image)['image']

        # Demografik √∂zellikler (EXPANDED - 12 features)
        demographics = self._extract_demographic_features(row)
        demographics = torch.FloatTensor(demographics)

        # Etiketler (MULTI-LABEL)
        labels = self._extract_labels(row)
        labels = torch.FloatTensor(labels)

        return {
            'image': image,
            'demographics': demographics,
            'labels': labels,
            'image_id': img_filename
        }

    def _find_image_path(self, img_filename):
        """G√∂r√ºnt√ºy√º images_XXX/images/ klas√∂rlerinde ara"""
        for img_dir in self.image_dirs:
            img_path = img_dir / img_filename
            if img_path.exists():
                return img_path

        raise FileNotFoundError(f"Bulunamadƒ±: {img_filename}")

    def _extract_demographic_features(self, row):
        """
        Demografik √∂zellikleri √ßƒ±kar ve normalize et
        Output: 12 features
        [age_norm, age_log, age_squared, age_bin1, age_bin2, age_bin3, age_bin4,
         gender_M, gender_F, view_PA, view_AP, view_other]
        """
        features = []

        # Ya≈ü √∂zellikleri (4 adet)
        age = row['Patient Age']
        features.append(age / 100.0)  # Normalize [0, 1]
        features.append(np.log1p(age) / np.log1p(100))  # Log transform
        features.append((age / 100.0) ** 2)  # Squared term

        # Ya≈ü gruplarƒ± (4 bins)
        features.append(1.0 if age < 18 else 0.0)    # √áocuk
        features.append(1.0 if 18 <= age < 45 else 0.0)  # Gen√ß yeti≈ükin
        features.append(1.0 if 45 <= age < 65 else 0.0)  # Orta ya≈ü
        features.append(1.0 if age >= 65 else 0.0)   # Ya≈ülƒ±

        # Cinsiyet (one-hot, 2 adet)
        features.append(1.0 if row['Patient Gender'] == 'M' else 0.0)
        features.append(1.0 if row['Patient Gender'] == 'F' else 0.0)

        # View position (one-hot, 3 adet)
        view = row['View Position']
        features.append(1.0 if view == 'PA' else 0.0)
        features.append(1.0 if view == 'AP' else 0.0)
        features.append(1.0 if view not in ['PA', 'AP'] else 0.0)  # L, LL, etc.

        return np.array(features, dtype=np.float32)

    def _extract_labels(self, row):
        """
        Multi-label etiketleri √ßƒ±kar
        Output: 15 boyutlu binary vector

        √ñNEMLƒ∞: Bir g√∂r√ºnt√ºde birden fazla hastalƒ±k olabilir!
        √ñrnek: "Infiltration|Effusion" ‚Üí [0,1,1,0,0,0,0,0,0,0,0,0,0,0,0]
        """
        labels = np.zeros(len(self.diseases), dtype=np.float32)
        findings = row['Finding Labels']

        if findings == 'No Finding':
            # Sadece 'No Finding' varsa, ilk label'ƒ± 1 yap
            labels[0] = 1.0
        else:
            # Multi-label: birden fazla hastalƒ±k olabilir
            disease_list = findings.split('|')
            for disease in disease_list:
                disease = disease.strip()
                if disease in self.diseases:
                    idx = self.diseases.index(disease)
                    labels[idx] = 1.0

        return labels

    def _print_label_statistics(self):
        """Multi-label istatistiklerini yazdƒ±r"""
        if self.mode != 'train':
            return

        print(f"\nüìä {self.mode.upper()} Multi-label Statistics:")

        # Her hastalƒ±ƒüƒ±n sayƒ±sƒ±nƒ± hesapla
        disease_counts = {disease: 0 for disease in self.diseases}
        multi_label_count = 0

        for _, row in self.df.iterrows():
            findings = row['Finding Labels']
            if findings != 'No Finding':
                diseases = findings.split('|')
                if len(diseases) > 1:
                    multi_label_count += 1
                for disease in diseases:
                    disease = disease.strip()
                    if disease in disease_counts:
                        disease_counts[disease] += 1
            else:
                disease_counts['No Finding'] += 1

        print(f"  Multi-label √∂rnekler: {multi_label_count} / {len(self.df)} "
              f"({multi_label_count/len(self.df)*100:.1f}%)")

        print(f"\n  Hastalƒ±k daƒüƒ±lƒ±mƒ±:")
        for disease, count in disease_counts.items():
            pct = count / len(self.df) * 100
            print(f"    {disease:20s}: {count:5,} ({pct:5.1f}%)")

    def _get_transforms(self):
        """Data augmentation ve normalization"""
        if self.mode == 'train' and config.USE_AUGMENTATION:
            strength = config.AUGMENTATION_STRENGTH

            if strength == 'light':
                # Hafif augmentation
                return A.Compose([
                    A.Resize(config.IMG_SIZE, config.IMG_SIZE),
                    A.HorizontalFlip(p=0.5),
                    A.Rotate(limit=10, p=0.3),
                    A.RandomBrightnessContrast(
                        brightness_limit=0.2,
                        contrast_limit=0.2,
                        p=0.3
                    ),
                    A.Normalize(
                        mean=config.NORMALIZE_MEAN,
                        std=config.NORMALIZE_STD
                    ),
                    ToTensorV2()
                ])

            elif strength == 'medium':
                # Orta seviye augmentation (√ñNERƒ∞LEN)
                return A.Compose([
                    A.Resize(config.IMG_SIZE, config.IMG_SIZE),

                    # Geometrik
                    A.HorizontalFlip(p=0.5),
                    A.Rotate(limit=15, p=0.4),
                    A.ShiftScaleRotate(
                        shift_limit=0.1,
                        scale_limit=0.1,
                        rotate_limit=15,
                        p=0.4
                    ),

                    # Piksel seviye
                    A.RandomBrightnessContrast(
                        brightness_limit=0.25,
                        contrast_limit=0.25,
                        p=0.5
                    ),
                    A.CLAHE(
                        clip_limit=2.0,
                        tile_grid_size=(8, 8),
                        p=0.3
                    ),
                    A.GaussNoise(
                        var_limit=(5.0, 20.0),
                        p=0.2
                    ),
                    A.CoarseDropout(
                        max_holes=5,
                        max_height=16,
                        max_width=16,
                        min_holes=2,
                        min_height=8,
                        min_width=8,
                        fill_value=0,
                        p=0.2
                    ),

                    A.Normalize(
                        mean=config.NORMALIZE_MEAN,
                        std=config.NORMALIZE_STD
                    ),
                    ToTensorV2()
                ])

            else:  # 'heavy'
                # Aƒüƒ±r augmentation (overfitting √ßok fazlaysa)
                return A.Compose([
                    A.Resize(config.IMG_SIZE, config.IMG_SIZE),
                    A.HorizontalFlip(p=0.5),
                    A.Rotate(limit=20, p=0.5),
                    A.ShiftScaleRotate(
                        shift_limit=0.15,
                        scale_limit=0.15,
                        rotate_limit=20,
                        p=0.5
                    ),
                    A.RandomBrightnessContrast(
                        brightness_limit=0.3,
                        contrast_limit=0.3,
                        p=0.5
                    ),
                    A.CLAHE(
                        clip_limit=3.0,
                        tile_grid_size=(8, 8),
                        p=0.4
                    ),
                    A.GaussNoise(
                        var_limit=(5.0, 30.0),
                        p=0.3
                    ),
                    A.GaussianBlur(
                        blur_limit=(3, 5),
                        p=0.2
                    ),
                    A.CoarseDropout(
                        max_holes=8,
                        max_height=20,
                        max_width=20,
                        min_holes=3,
                        min_height=8,
                        min_width=8,
                        fill_value=0,
                        p=0.3
                    ),
                    A.Normalize(
                        mean=config.NORMALIZE_MEAN,
                        std=config.NORMALIZE_STD
                    ),
                    ToTensorV2()
                ])
        else:
            # Val/Test: Sadece normalize
            return A.Compose([
                A.Resize(config.IMG_SIZE, config.IMG_SIZE),
                A.Normalize(
                    mean=config.NORMALIZE_MEAN,
                    std=config.NORMALIZE_STD
                ),
                ToTensorV2()
            ])


def create_dataloaders(train_csv, val_csv, test_csv, img_dir, batch_size):
    """
    Train/Val/Test dataloader'larƒ±nƒ± olu≈ütur
    """
    print("\n" + "="*70)
    print("üì¶ DATALOADER'LAR OLU≈ûTURULUYOR")
    print("="*70)

    # Dataset'leri olu≈ütur
    train_dataset = ChestXrayMultimodalDataset(train_csv, img_dir, mode='train')
    val_dataset = ChestXrayMultimodalDataset(val_csv, img_dir, mode='val')
    test_dataset = ChestXrayMultimodalDataset(test_csv, img_dir, mode='test')

    # DataLoader'lar
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,  # Random shuffle
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY,
        drop_last=True,
        prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY,
        prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None
    )

    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=config.PIN_MEMORY,
        prefetch_factor=config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None
    )

    print(f"\n‚úì DataLoader'lar hazƒ±r:")
    print(f"  - Train batches: {len(train_loader):,}")
    print(f"  - Val batches: {len(val_loader):,}")
    print(f"  - Test batches: {len(test_loader):,}")
    print("="*70)

    return train_loader, val_loader, test_loader


if __name__ == '__main__':
    # Test
    print("Dataset test ediliyor...")

    # Test i√ßin dummy CSV olu≈ütur
    test_data = {
        'Image Index': ['00000001_000.png', '00000002_000.png'],
        'Finding Labels': ['No Finding', 'Infiltration|Effusion'],
        'Patient Age': [58, 45],
        'Patient Gender': ['M', 'F'],
        'View Position': ['PA', 'AP']
    }
    test_df = pd.DataFrame(test_data)
    test_df.to_csv('/tmp/test.csv', index=False)

    dataset = ChestXrayMultimodalDataset(
        csv_file='/tmp/test.csv',
        img_dir='/kaggle/input/nih-chest-xrays/data',
        mode='train'
    )

    print(f"\nDataset size: {len(dataset)}")
    print(f"Diseases: {dataset.diseases}")
